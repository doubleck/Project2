---
title: "STA6106 Statistic Computing Project2"
author: "Robert Norberg, Jung-Han Wang"
date: "Wednesday, October 22, 2014"
output: html_document
---

```{r ChunkSettings, echo=FALSE, warning=FALSE, message=FALSE, results='hide'}
# Clear working environment
rm(list=ls())

# Options for document compilation
knitr::opts_chunk$set(warning=FALSE, message=FALSE, comment=NA, fig.width=4, fig.height=3)
```

## Problem1

The function g is given by 

$$ g(x,y) = 4xy + (x + y^2)^2 $$


The goal is to find the minimum of g.

### a. Minimize g using Newton's method

The Newton's Algorithm is: $$\pmb{x(n+1)=x(n)-H(x(n))^{-1}\bigtriangledown f(x(n))}$$

First step we need to verify that $H(x(n))$ is non-singular.

```{r}
hx<-D(D(expression(4*x*y+(x+y^2)^2), 'x'),'x');hx ## Check H(X(n)) is a non-singular matrix
```

Setting up stopping condition to be: $$\pmb{\|\bigtriangledown f(x(n))\|} \le \epsilon $$


```{r}
newton <- function(f3, x0, tol = 1e-9, n.max = 100) {
# Newton's method starting at x0
# f3 is a function that given x returns the list
# f(x), f'(x), Hessian f''(x)
x <- x0  ## Set initial value
f3.x <- f3(x) ## Set Input Function
n <- 0 ## Set first turn n<-0
while ((max(abs(f3.x[[2]])) > tol) & (n < n.max)) { ##Set Convergence Criteria. If f'(x) greater than tol(tolerance) go to n+1.
x <- x - solve(f3.x[[3]], f3.x[[2]]) ##Calculate f'(x)/f''(x)
f3.x <- f3(x) 
n <- n + 1 ##Continue to the next n
}
if (n == n.max) { ##If n=maximum value, output "newton failed to converge"
cat('newton failed to converge\n')
} else {
return(x)
}
}
```

Then Calculate first and second derivatives for function $g(x,y) = 4xy + (x + y^2)^2$ Using package (Ryacas).

```{r}
library(Ryacas);
k <- function(x) {
4*x[1]*x[2]+(x[1]+x[2]^2)^2
}
##Calculate First Derivative of Function to X  (f1)
f1<-D(expression(4*x*y+(x+y^2)^2), 'x');f1
##Calculate Second Derivative of Function to X (f11)
f11<-D(D(expression(4*x*y+(x+y^2)^2), 'x'),'x');f11
##Calculate First Derivative of Function to y  (f2)
f2<-D(expression(4*x*y+(x+y^2)^2), 'y');f2
##Calculate Second Derivative of Function to X (f22)
f22<-D(D(expression(4*x*y+(x+y^2)^2), 'y'),'y');f22
##Calculate Second Derivative of Function to X,y (f12)
f12<-D(D(expression(4*x*y+(x+y^2)^2), 'x'),'y');f12
```

Set up original function and first derivative and second derivative.

```{r}
f3 <- function(x) {
f <- 4*x[1]*x[2]+(x[1]+x[2]^2)^2
##Calculate First Derivative of Function to x[1]  (f1)
f1<-4*x[2]+2*(x[1] + x[2]^2)
##Calculate Second Derivative of Function to x[1] (f11)
f11<-2
##Calculate First Derivative of Function to x[2]  (f2)
f2<-4 * x[1] + 2 * (2 * x[2] * (x[1] + x[2]^2))
##Calculate Second Derivative of Function to x[1] (f22)
f22<-2 * (2 * (x[1] + x[2]^2) + 2 * x[2] * (2 * x[2]))
##Calculate Second Derivative of Function to x[1],x[2] (f12)
f12<-4 + 2 * (2 * x[2])
return(list(f, c(f1, f2), matrix(c(f11, f12, f12, f22), 2, 2)))  ##Return 3 values, f, f'(x), f''(x)
}


for (x0 in 0) {
for (y0 in 1) {
cat(c(x0,y0), '-->', newton(f3, c(x0,y0)), '\n')
}}

```






### b. Minimize g using the steepest descent method. Use (1,0) as starting point.

```{r}

```


## Problem2

In 1986, the space shuttle Challenger exploded during takeoff, killing the seven astronauts aboard. The explosion was the result of an O-ring failure, a splitting of a ring of rubber that seals the parts of the ship together. The accident was believed to have been caused by the unusually cold weather ($31^\circ F$ or $0^\circ C$) at the time of launch, as there is reason to believe that the O-ring failure probabilities increase as temperature decreases. Data on previous space shuttle launches and O-ring failures is given in the dataset challenger provided with the "mcsm" package of R. The first column corresponds to the failure indicators $y_i$ and the second column to the corresponding temperature $x_i$, (1$\le$i$\le$  24).

```{r}
# load challenger data
library(mcsm)
data(challenger)
```


### a) The goal is to obtain MLEs for $\beta_0$ and $\beta_1$ in the following logistic regression model

$$log \left( \dfrac{p}{1-p} \right) = \beta_0+\beta_1x$$

where $p$ is the probability that at least one O-ring is damaged and $x$
is the temperature. Create computer programs using Newton-Raphson algorithm to find MLEs of $\widehat{\beta}_0, \widehat{\beta}_1$

The log-likelihood is given by

$$l(\beta) = \pmb{y}^T \pmb{Z \beta}-\pmb{b}^T \pmb{1}$$

where $\pmb{1}$ is a column vector of ones, $\pmb{y} = (y_1...y_n)^T$, $\pmb{b} = (b(\theta_1)...b(\theta_n))^T$, and $\pmb{Z}$ is the $n\, X\, 2$ matrix whose $i$ th row is $\pmb{z}_i^T$.

The score function is 

$$\pmb{l}'(\pmb{\beta})=\pmb{Z}^T(\pmb{y}-\pmb{\pi})$$

where $\pmb{\pi}$ is a column vector of the Bernoulli probabilities $\pi_1, ..., \pi_n$. The Hessian is given by 

$$\pmb{l}''(\pmb{\beta}) = \dfrac{d}{d \pmb{\beta}}(\pmb{Z}^T(\pmb{y}-\pmb{\pi})) = -\left( \dfrac{d \pmb{\pi}}{d \pmb{\beta}} \right)^T \pmb{Z} = -\pmb{Z}^T\pmb{WZ}$$

where $\pmb{W}$ is a diagonal matrix with $i$ th diagonal entry equal to $\pi_i(1-\pi_i)$.

Newton's update is therefore 

$$\pmb{\beta}^{(t+1)} = \pmb{\beta}^{(t)}-\pmb{l}''(\pmb{\beta}^{(t)})^{-1}\pmb{l}'(\pmb{\beta}^{(t)}) = \pmb{\beta}^{(t)}+\left( \pmb{Z}^T\pmb{W}^{(t)}\pmb{Z}\right)^{-1}\left(\pmb{Z}^T(\pmb{y}-\pmb{\pi}^{(t)})\right)$$

where $\pmb{\pi}^{(t)}$ is the value of $\pmb{\pi}$ corresponding to $\pmb{\beta}^{(t)}$, and $\pmb{W}^{(t)}$ is the diagonal weight matrix evaluated at $\pmb{\pi}^{(t)}$.

To find MLEs of $\widehat{\beta}_0$ and $\widehat{\beta}_1$ we designate the vector $\pmb{y}$ to be the response vector in the data and $\pmb{Z}$ to be the $n\, X\, 2$ matrix of ones and temperatures.
```{r}
y <- challenger$oring
x <- challenger$temp
Z <- cbind(rep(1, nrow(challenger)), challenger$temp)
```

$\pmb{Z}$ will remain unchanged, but for each iteration we will need to compute the predicted probabilities $\pi_1, ...\pi_n$ using the current values of $\widehat{\beta}_0$ and $\widehat{\beta}_1$. We will also need to compute for each iteration $\pmb{W}$, the diagonal matrix with diagonal entry $i$ equal to $\pi_i(1-\pi_1)$. Below, we define a function to do each of these computations.
```{r}
find_pis <- function(beta, x){
  # beta a vector of parameter estimates, x a vector of independent variable observations
  pis <- exp(beta%*%t(Z))/(1+exp(beta%*%t(Z)))
  return(pis)
}

find_W <- function(pis){
  W <- diag(c(pis*(1-pis)))
  return(W)
}
```

Next we difine a function to calculate Newton's update using $\pmb{Z}$, $\pmb{y}$, $\pmb{\beta}$, and the $\pi s$ and $\pmb{W}$ calculated using the functions above.
```{r}
find_update <- function(beta, Z, W, y, pis){
  numerator <- t(Z)%*%(y-t(pis))
  denominator <- t(Z)%*%W%*%Z
  update <- solve(denominator)%*%numerator
  return(beta+c(update))
}
```

Finally, we must define a stopping criterion so that the algorithm ceases to update at some point and returns to us values for $\widehat{\beta}_0$ and $\widehat{\beta}_1$. We will stop updating our estimates when an iteration takes place that does not change them. We are not concerned with accuracy at more than seven decimal places, but we will write the program so that this may be changed.

We choose startting values for $\widehat{\beta}_0$ and $\widehat{\beta}_1$ to be $[0, 0]$, as is reccomended in the textbook. Then we begin a while loop to update these estimates until convergence occurs.
```{r}
beta <- c(0, 0)
old_beta <- c(NA, NA)
counter <- 0
decimal_places_of_precision <- 7
verbose <- TRUE

while(!identical(beta, old_beta)){
  pis <- find_pis(beta, x)
  W <- find_W(pis)
  # set old beta equal to current beta before updating
  old_beta <- beta
  # replace beta with updated estimates
  beta <- find_update(beta, Z, W, y, pis)
  # round to the number of decimal places desired
  beta <- round(beta, decimal_places_of_precision)
  counter <- counter+1
  # show us the current value of beta if "verbose" is TRUE
  if(verbose==T){
    msg <- paste0('Iteration ', counter, ': Beta = (', beta[1], ', ', beta[2], ')')
  cat(msg)
  cat('\n')
  }
}
```

Now that the while loop has terminated, the current values of `beta` should be the MLEs of $\widehat{\beta}_0$ and $\widehat{\beta}_1$. We check these against the estimates given using the `glm()` function in R.
```{r}
# see our estimates
print(beta)
# see the glm estimates
glm(y~x, family=binomial)$coefficients
```

Our estimates match those produced by the `glm()` function, so we are confident that we have correctly found the MLEs of $\widehat{\beta}_0$ and $\widehat{\beta}_1$.

### b) Solve the same problem using the "Iterative Reweighted Least Squares" algorithm and the Newton-Raphson algorithm to find MLEs of $\widehat{\beta}_0, \widehat{\beta}_1$

If we let 

$$\pmb{e}^{(t)} = \pmb{y}-\pmb{\pi}^{(t)}$$

and

$$\pmb{x}^{(t)} = \pmb{Z\beta}^{(t)} + (\pmb{W}^{(t)})^{-1}\pmb{e}^{(t)}$$

then the Fisher scoring update can be written as 

$$\pmb{\beta}^{(t+1)} = \pmb{\beta}+\left( \pmb{Z}^T\pmb{W}^{(t)}\pmb{Z} \right)^{-1} \pmb{Z}^T\pmb{e}^{(t)}$$
$$ = \left( \pmb{Z}^T\pmb{W}^{(t)}\pmb{Z}\right)^{-1}\pmb{Z}^T\pmb{W}^{(t)}\pmb{x}^{(t)}$$

where $\pmb{x}^{(t)}$ is the *working response* that gets updated with eac iteration.

If we reuse the functions defined earlier to calculate $\pmb{\pi}$ and $\pmb{W}$, we just need to define a function for calculating $\pmb{x}$ and define a new function for updating using the iteratively reweighted least squares update before we begin to iterate. We define a function for calculating $\pmb{x}$ as follows:
```{r}
find_x <- function(y, beta, pis, Z, W){
  e_t <- y-c(pis)
  x <- Z%*%beta+solve(W)%*%e_t
  return(x)
}
```

And a new updating function:
```{r}
find_update_irls <- function(Z, W, working_response){
  numerator <- t(Z)%*%W%*%working_response
  denominator <- t(Z)%*%W%*%Z
  update <- solve(denominator)%*%numerator
}
```

Next we choose $(0, \, 0)$ for the initial values of $\pmb{\beta}$ and begin another while loop, this time updating using the iteratively reweighted least squares update. (We are still satisfied with 7 decimal places of accuracy.)
```{r}
beta <- c(0, 0)
old_beta <- c(NA, NA)
counter <- 0
decimal_places_of_precision <- 7
verbose <- TRUE

while(!identical(beta, old_beta)){
  pis <- find_pis(beta, x)
  W <- find_W(pis)
  working_response <- find_x(y, beta, pis, Z, W)
  # set old beta equal to current beta before updating
  old_beta <- beta
  # replace beta with updated estimates
  beta <- find_update_irls(Z, W, working_response)
  # round to the number of decimal places desired
  beta <- round(c(beta), decimal_places_of_precision)
  counter <- counter+1
  # show us the current value of beta if "verbose" is TRUE
  if(verbose==T){
    msg <- paste0('Iteration ', counter, ': Beta = (', beta[1], ', ', beta[2], ')')
  cat(msg)
  cat('\n')
  }
}
```

We arrive at the same MLEs as before, in the same number of iterations as before.

### c) We are also interested in predicting O-ring failure. Challenger was launched at $31^\circ F$. What is the predicted probability of O-ring damage at $31^\circ F$ ? How many O-ring failures should be expected at $31^\circ F$ ? What can you conclude?

At $31^\circ F$ we predict the probability of at least one O-ring failure to be 

$$p = \dfrac{e^{\beta_0 + (\beta_1)(31)}}{1+e^{\beta_0 + (\beta_1)(31)}} = \dfrac{e^{15.0429 + (-0.2322)(31)}}{1+e^{15.0429 + (-0.2322)(31)}} = 0.9996$$

This prediction means that we are almost certain that at least one O-ring will fail at $31^\circ F$.

## Problem 3

The elastic net (Zou and Hastie, 2006) is considered to be a compromise between the ridge and lasso penalties. The elastic net can be formulated using the Lagrangian as follows:

$$\widehat{\beta}^{enet} = \underset{\beta}{\mathrm{argmin}}\sum\limits_{i=1}^{n}(y_i-x'_1 \beta)^2 + \lambda_1 \sum\limits_{j=1}^{p} |\beta_j| + \lambda_2 \sum\limits_{j=1}^{p}\beta_j^2$$

where $\lambda_1 \ge 0$ and $\lambda_2 \ge 0$.
The "credit" data set is discussed in the textbook of James et al., p83. We will fit the elastic net model to the "credit" data set using only the quantitative predictors. Our challenge is to select the appropriate $\lambda_1$ and $\lambda_2$ before fitting the final model.

### a) Write a function in R using the cross-validation approach to find the optimum values of $\lambda_1$ and $\lambda_2$

### b) Repeat the same question as in (a) but using now the *one-standard-error* (1â€“SE) rule cross validation

## Appendix with R code

```{r all-code, ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE, tidy.opts=list(keep.blank.line=T)}
```

-----

